{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to NLP and Text Processing\n",
        "Welcome to the first session on Natural Language Processing (NLP). In this notebook, we will cover:\n",
        "- Overview of NLP\n",
        "- Techniques for text normalization\n",
        "- Basics of tokenization and stopword removal\n",
        "- Implementing Stemming, Lemmatization\n"
      ],
      "metadata": {
        "id": "gzi39TwROt7B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is NLP?\n",
        "Natural Language Processing (NLP) is a field of Artificial Intelligence (AI) focused on enabling machines to understand, interpret, and generate human language.\n",
        "\n",
        "### Applications of NLP:\n",
        "- Sentiment Analysis\n",
        "- Chatbots\n",
        "- Machine Translation\n",
        "- Text Summarization\n",
        "- Speech Recognition\n"
      ],
      "metadata": {
        "id": "F6DHtInoPZop"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text Normalization\n",
        "Text normalization involves cleaning and standardizing text data. Common steps include:\n",
        "1. Lowercasing\n",
        "2. Removing punctuation\n",
        "3. Expanding contractions\n",
        "4. Removing special characters\n"
      ],
      "metadata": {
        "id": "hM0XzfiGPdep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "\n",
        "# Example Text\n",
        "text = \"Hello, World! This is an NLP class. Let's explore TEXT processing. i am also doing a B.d, i bught this pen for 9.99$. \"\n",
        "\n",
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKPpvS6XNcEa",
        "outputId": "fb4e73b6-7183-46e3-df24-bb7547581747"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello,',\n",
              " 'World!',\n",
              " 'This',\n",
              " 'is',\n",
              " 'an',\n",
              " 'NLP',\n",
              " 'class.',\n",
              " \"Let's\",\n",
              " 'explore',\n",
              " 'TEXT',\n",
              " 'processing.']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hello world this is an nlp class lets explore text processing\"\n",
        "text.split()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jlVPLyQOFFl",
        "outputId": "d293e016-3bee-4e09-e122-d579f0024b00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello',\n",
              " 'world',\n",
              " 'this',\n",
              " 'is',\n",
              " 'an',\n",
              " 'nlp',\n",
              " 'class',\n",
              " 'lets',\n",
              " 'explore',\n",
              " 'text',\n",
              " 'processing']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "# Example Text\n",
        "text = \"Hello, World! This is an NLP class. Let's explore TEXT processing.\"\n",
        "\n",
        "# Lowercasing\n",
        "text_lower = text.lower()\n",
        "\n",
        "# Removing punctuation\n",
        "text_no_punct = re.sub(r'[^\\w\\s]', '', text_lower)\n",
        "\n",
        "# Result\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Lowercased Text:\", text_lower)\n",
        "print(\"Text without Punctuation:\", text_no_punct)\n"
      ],
      "metadata": {
        "id": "nIvPAnPnOxCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbbae294-2c11-40c9-dce1-2ae01ff3aaf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: Hello, World! This is an NLP class. Let's explore TEXT processing.\n",
            "Lowercased Text: hello, world! this is an nlp class. let's explore text processing.\n",
            "Text without Punctuation: hello world this is an nlp class lets explore text processing\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "Tokenization is the process of breaking down text into smaller units called tokens. Tokens can be words, sentences, or characters.\n"
      ],
      "metadata": {
        "id": "oZqFcxWtPjs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "# Example Text\n",
        "text = \"Tokenization splits text into meaningful pieces. It's a crucial step in NLP!\"\n",
        "\n",
        "# Word Tokenization\n",
        "word_tokens = word_tokenize(text)\n",
        "\n",
        "# Sentence Tokenization\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "\n",
        "print(\"Word Tokens:\", word_tokens)\n",
        "print(\"Sentence Tokens:\", sentence_tokens)\n"
      ],
      "metadata": {
        "id": "rteVwTOCPkdZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "344a9781-5ee1-45da-f3f6-4c16fafd3b37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['Tokenization', 'splits', 'text', 'into', 'meaningful', 'pieces', '.', 'It', \"'s\", 'a', 'crucial', 'step', 'in', 'NLP', '!']\n",
            "Sentence Tokens: ['Tokenization splits text into meaningful pieces.', \"It's a crucial step in NLP!\"]\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GcqFxKyUgqtc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stopword Removal\n",
        "Stopwords are common words (e.g., \"is\", \"the\", \"in\") that are usually removed to focus on meaningful words.\n"
      ],
      "metadata": {
        "id": "QXzzaFzNPrdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopword = \"i\n",
        "me\n",
        "my\n",
        "myself\n",
        "we\n",
        "our\n",
        "ours\n",
        "ourselves\n",
        "you\n",
        "your\n",
        "yours\n",
        "yourself\n",
        "yourselves\n",
        "he\n",
        "him\n",
        "his\n",
        "himself\n",
        "she\n",
        "her\n",
        "hers\n",
        "herself\n",
        "it\n",
        "its\n",
        "itself\n",
        "they\n",
        "them\n",
        "their\n",
        "theirs\n",
        "themselves\n",
        "what\n",
        "which\n",
        "who\n",
        "whom\n",
        "this\n",
        "that\n",
        "these\n",
        "those\n",
        "am\n",
        "is\n",
        "are\n",
        "was\n",
        "were\n",
        "be\n",
        "been\n",
        "being\n",
        "have\n",
        "has\n",
        "had\n",
        "having\n",
        "do\n",
        "does\n",
        "did\n",
        "doing\n",
        "a\n",
        "an\n",
        "the\n",
        "and\n",
        "but\n",
        "if\n",
        "or\n",
        "because\n",
        "as\n",
        "until\n",
        "while\n",
        "of\n",
        "at\n",
        "by\n",
        "for\n",
        "with\n",
        "about\n",
        "against\n",
        "between\n",
        "into\n",
        "through\n",
        "during\n",
        "before\n",
        "after\n",
        "above\n",
        "below\n",
        "to\n",
        "from\n",
        "up\n",
        "down\n",
        "in\n",
        "out\n",
        "on\n",
        "off\n",
        "over\n",
        "under\n",
        "again\n",
        "further\n",
        "then\n",
        "once\n",
        "here\n",
        "there\n",
        "when\n",
        "where\n",
        "why\n",
        "how\n",
        "all\n",
        "any\n",
        "both\n",
        "each\n",
        "few\n",
        "more\n",
        "most\n",
        "other\n",
        "some\n",
        "such\n",
        "no\n",
        "nor\n",
        "not\n",
        "only\n",
        "own\n",
        "same\n",
        "so\n",
        "than\n",
        "too\n",
        "very\n",
        "s\n",
        "t\n",
        "can\n",
        "will\n",
        "just\n",
        "don\n",
        "should\n",
        "now\n",
        "\""
      ],
      "metadata": {
        "id": "BJQCQjZBSBvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Example Text\n",
        "text = \"This is a simple example to demonstrate stopword removal. is there are we \"\n",
        "\n",
        "# Tokenize Text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Load Stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Remove Stopwords\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "print(\"Original Tokens:\", tokens)\n",
        "print(\"Filtered Tokens (Without Stopwords):\", filtered_tokens)\n",
        "\n"
      ],
      "metadata": {
        "id": "hVQprsrbPoMh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82a37bf4-176f-4e16-e543-13e4f498610e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Tokens: ['This', 'is', 'a', 'simple', 'example', 'to', 'demonstrate', 'stopword', 'removal', '.', 'is', 'there', 'are', 'we']\n",
            "Filtered Tokens (Without Stopwords): ['simple', 'example', 'demonstrate', 'stopword', 'removal', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVYeeH8aSdMP",
        "outputId": "4ef96473-cbf5-4beb-9add-d2db1922c07e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'hdfjhfbd',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " \"he's\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " \"i've\",\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " \"we've\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise\n",
        "1. Write a function to normalize text (lowercase, remove punctuation).\n",
        "2. Tokenize the normalized text.\n",
        "3. Remove stopwords from the tokens.\n",
        "4. Test your function with a sample paragraph.\n"
      ],
      "metadata": {
        "id": "lVE2BiOqPwox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "    # Normalize\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Remove Stopwords\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    return filtered_tokens\n",
        "\n",
        "# Test with Example Text\n",
        "sample_text = \"\"\"Artificial intelligence (AI) refers to the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1] Such machines may be called AIs.\"\"\"\n",
        "print(preprocess_text(sample_text))\n"
      ],
      "metadata": {
        "id": "wtjzY2KdPxQN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67077de9-8823-44f0-f680-e6d01eda7996"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['artificial', 'intelligence', 'ai', 'refers', 'capability', 'computational', 'systems', 'perform', 'tasks', 'typically', 'associated', 'human', 'intelligence', 'learning', 'reasoning', 'problemsolving', 'perception', 'decisionmaking', 'field', 'research', 'computer', 'science', 'develops', 'studies', 'methods', 'software', 'enable', 'machines', 'perceive', 'environment', 'use', 'learning', 'intelligence', 'take', 'actions', 'maximize', 'chances', 'achieving', 'defined', 'goals1', 'machines', 'may', 'called', 'ais']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: Text Preprocessing for Sentiment Analysis in NLP\n",
        "This case study focuses on applying various preprocessing techniques to a real-world dataset: the IMDB Movie Reviews Dataset. The dataset contains 50,000 movie reviews labeled as positive or negative, making it ideal for exploring text preprocessing and its impact on data quality.\n",
        "\n",
        "**Objective**\n",
        "To preprocess the IMDB movie reviews dataset using standard NLP techniques, including:\n",
        "\n",
        "*   Text Normalization   \n",
        "*   Tokenization\n",
        "*   Stopword Removal\n",
        "\n",
        "The processed data will be analyzed to understand its structure and utility for downstream NLP tasks such as sentiment analysis."
      ],
      "metadata": {
        "id": "XteyoEjWQdCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset**\n",
        "The IMDB Movie Reviews Dataset consists of the following:\n",
        "\n",
        "**Columns:**\n",
        "* **text**: The movie review text.\n",
        "* **label**: Sentiment label (positive or negative).\n",
        "\n",
        "**Source:** [Kaggle IMDB Dataset.](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews)\n",
        "\n",
        "The dataset can also be loaded programmatically using the datasets library in Python."
      ],
      "metadata": {
        "id": "0JGCQyW5RBeJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Methodology\n",
        "1. Loading and Exploring the Dataset\n",
        "\n",
        "The dataset is loaded into a pandas DataFrame for inspection and preprocessing. The structure of the data is explored to understand the distribution of positive and negative reviews."
      ],
      "metadata": {
        "id": "HW7n6SswRbW5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the IMDB dataset\n",
        "dataset = load_dataset('imdb')\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "\n",
        "# Inspect the dataset\n",
        "print(df.head())\n",
        "print(\"Dataset Shape:\", df.shape)\n",
        "print(\"Label Distribution:\\n\", df['label'].value_counts())\n"
      ],
      "metadata": {
        "id": "PyJNpMrxQ9Xx"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Text Normalization\n",
        "Normalization involves cleaning the text to make it consistent. Key steps include:\n",
        "\n",
        "* Converting text to lowercase.\n",
        "* Removing punctuation and special characters."
      ],
      "metadata": {
        "id": "Oh360KnuRohQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def normalize_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation and special characters\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text\n",
        "\n",
        "# Apply normalization\n",
        "df['normalized_text'] = df['text'].apply(normalize_text)\n"
      ],
      "metadata": {
        "id": "1M2MBxS0RmYg"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Tokenization\n",
        "Tokenization splits the text into smaller units, typically words or sentences. For this study, word tokenization is applied to each review."
      ],
      "metadata": {
        "id": "n39ksZweRw1g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "\n",
        "# Apply tokenization\n",
        "df['tokens'] = df['normalized_text'].apply(tokenize_text)\n"
      ],
      "metadata": {
        "id": "Wg5Jvb5LRy9Z"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Stopword Removal\n",
        "Stopwords (e.g., \"the\", \"is\", \"and\") are common words that carry little semantic meaning. Removing them helps focus on meaningful content."
      ],
      "metadata": {
        "id": "AmiADBb1R2Ax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "\n",
        "# Apply stopword removal\n",
        "df['filtered_tokens'] = df['tokens'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "yvVjYjYMR3_J"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Analysis and Visualization\n",
        "To evaluate preprocessing, we calculate word counts before and after preprocessing and visualize the most common words in the dataset."
      ],
      "metadata": {
        "id": "d_ODTqRTR9CI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['original_word_count'] = df['text'].apply(lambda x: len(x.split()))\n",
        "df['processed_word_count'] = df['filtered_tokens'].apply(len)\n",
        "\n",
        "# Compare average word counts\n",
        "print(\"Average Word Count (Original):\", df['original_word_count'].mean())\n",
        "print(\"Average Word Count (Processed):\", df['processed_word_count'].mean())\n"
      ],
      "metadata": {
        "id": "h607DdT-R-mI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualization: Using a bar chart, we display the most frequent words in the processed reviews.**"
      ],
      "metadata": {
        "id": "3tDB6gmVSDC5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Count word frequencies\n",
        "all_words = [word for tokens in df['filtered_tokens'] for word in tokens]\n",
        "word_counts = Counter(all_words)\n",
        "common_words = word_counts.most_common(10)\n",
        "\n",
        "# Plot the most common words\n",
        "words, counts = zip(*common_words)\n",
        "plt.bar(words, counts)\n",
        "plt.title(\"Most Common Words in Processed Reviews\")\n",
        "plt.xlabel(\"Words\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "yIWJnZnqSGki"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Insights\n",
        "* Normalization Impact: Reducing case sensitivity and punctuation enhanced the dataset's consistency.\n",
        "* Tokenization Insights: Tokenization provided a granular look at the text, enabling more detailed analyses.\n",
        "* Stopword Removal Impact: Removing stopwords reduced noise in the data, focusing on words with higher semantic value.\n",
        "* Word Count Comparison: Preprocessing reduced the average word count, indicating noise removal."
      ],
      "metadata": {
        "id": "ZXzSQFkiSKah"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming, Lemmatization"
      ],
      "metadata": {
        "id": "jofLp5ll07dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk scikit-learn"
      ],
      "metadata": {
        "id": "YDcMsHby06Gi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "S0Ztc1q21LUC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install nltk scikit-learn\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "c2cCyv9o1HyJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "975b278a-0984-48f0-d6f3-ab924f2d495c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "o2Pr3BPD1KEJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58e39833-8f8e-4ff4-f2d3-c0ebd87dc37c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding and Applying Stemming and Lemmatization\n",
        "\n",
        "### Explanation:\n",
        "- **Stemming** reduces words to their root forms by chopping off suffixes (e.g., \"running\" -> \"run\").\n",
        "- **Lemmatization** maps words to their base forms using linguistic rules (e.g., \"better\" -> \"good\").\n"
      ],
      "metadata": {
        "id": "M4HBQpII6ye1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "# Example Text\n",
        "text = \"The striped bats are hanging on their feet for best\"\n",
        "\n",
        "# Stemming\n",
        "stemmer = PorterStemmer()\n",
        "stemmed_words = [stemmer.stem(word) for word in word_tokenize(text)]\n",
        "print(\"Stemmed Words:\", stemmed_words)\n",
        "\n",
        "# Lemmatization\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_words = [lemmatizer.lemmatize(word) for word in word_tokenize(text)]\n",
        "print(\"Lemmatized Words:\", lemmatized_words)"
      ],
      "metadata": {
        "id": "IUfWzU0z6a-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a260737-760f-4cef-dbd4-b63668550ad3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stemmed Words: ['the', 'stripe', 'bat', 'are', 'hang', 'on', 'their', 'feet', 'for', 'best']\n",
            "Lemmatized Words: ['The', 'striped', 'bat', 'are', 'hanging', 'on', 'their', 'foot', 'for', 'best']\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Case Study: Text Preprocessing"
      ],
      "metadata": {
        "id": "KJU5PHYp1enh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Background**\n",
        "Natural Language Processing (NLP) requires converting raw text into numerical features for machine learning models. This case study demonstrates the application of text preprocessing to prepare text data for analysis.\n",
        "\n",
        "---\n",
        "\n",
        "#### **Objective**\n",
        "- Preprocess text data (cleaning, tokenization, etc.).\n",
        "\n"
      ],
      "metadata": {
        "id": "dJkC-s1Z73yO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Dataset**\n",
        "For this case study, we use a sample corpus consisting of multiple sentences related to Natural Language Processing (NLP).\n"
      ],
      "metadata": {
        "id": "BLpqEAzN78CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample Corpus\n",
        "corpus = [\n",
        "    \"Natural Language Processing is a subfield of AI.\",\n",
        "    \"Deep Learning is part of machine learning.\",\n",
        "    \"NLP applications are amazing and diverse.\",\n",
        "    \"AI is transforming industries through NLP.\"\n",
        "]"
      ],
      "metadata": {
        "id": "ySX3YCTQ8BlF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Text Preprocessing**\n"
      ],
      "metadata": {
        "id": "bzasKfZU8Jl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Preprocessing Function\n",
        "def preprocess_text(text):\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove special characters and digits\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Apply stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    # Apply lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "# Apply preprocessing to the corpus\n",
        "preprocessed_corpus = [preprocess_text(doc) for doc in corpus]\n",
        "print(\"Preprocessed Corpus:\", preprocessed_corpus)"
      ],
      "metadata": {
        "id": "wTMtjWMK8KZ1"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}